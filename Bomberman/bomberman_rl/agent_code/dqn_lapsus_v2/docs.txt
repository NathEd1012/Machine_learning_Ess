Documentation for dqn_lapsus_v1:

In the first round, we want to train the agent to collect coins and then later to destroy crates and avoid its bombs:

    ################## Variant 1.1.1

        e.INVALID_ACTION: -0.05,
        e.MOVED_LEFT: 0.01,
        e.MOVED_RIGHT: 0.01,
        e.MOVED_UP: 0.01,
        e.MOVED_DOWN: 0.01,
        e.WAITED: -0.02,
        e.KILLED_SELF: -5,
        # e.BOMB_DROPPED: 0.05,
        e.SURVIVED_ROUND: 4,
        e.COIN_COLLECTED: 1,
        e.CRATE_DESTROYED: 0.3,
        # Custom events
        e.COOL: 0.01,
        e.WARM: -0.02,
        e.HOT: -0.05,
        e.BOILING: -0.1,
        e.FRESHENED_UP: 0.1
    
    ---------> Results: 
        
        #### First training session: (stats/ts-1-1-1.csv)

        The model was initially trained on 500 rounds of coin-heaven. The model learns consistently within 100+ rounds to gather all possible coins. In the play scenario, it always gathers all coins. The agent hasn't learned to
        survive bombs yet, it does mostly so by accident because it's so eager to move on to the next coin that it avoids the explosion. When there's a coin and in its
        path the explosion it blows itself up.

        After succesfully training it to gather coins, we started training on scenario loot-crate (500-9000). With the current reward system, it shows a slow improvement
        at destroying crates and has somewhat succesfully learned to place bombs and then run away from them. During play it can be seen performing
        the characteristic L to avoid the bombs it places.

        However, it seems that the agent learned some unwanted behaviors. Particularly, due to there being rewards for simultaneously placing bombs and
        also running out of danger, it often gets stuck in a loop where it will place a bomb, avoid it, and then place a bomb again. Moving on the same
        L. 

        Playing again on the scenario coin-heaven shows that the agent has unlearned many of the initial efficient pathfinding skills. It has learned that placing bombs is good,
        but is not as interested in getting the coins as much. It routinely kills itself accidentally after just a few coins. We tried re-training it on coin-heaven (9000-9500), 
        which leads it to both place a lot of bombs, avoid them succesfully and gather all the coins (in most cases). However, going back to loot-crate confuses the agent entirely.
        Removing a large part of the randomness, by training 500 rounds (9500-10000) with EPSILON = 0.1 -> 0.01 indicates that it mastered again to collect coins, but not drop any bombs.

        From these insights, we have some ideas on how to improve the next version.

    ################## Variant 1.2.1

        #### First training session: (stats/ts-1-2-1.csv)

            First: We need to introduce a penalty for bad bomb placement. If the agent places a bomb which doesn't break any crates
            it should be penalized.

            Second: We need to change the rewards to make the bomb-placement-escape unsuitable as a strategy.

            Third: We need to adjust the learning, so that the agent can act flexibly independent of the scenario: It could first learn to search for coins,
            then to destroy crates, but it should be able to go back towards a coin-only scenario.

        After training the agent for 2000 rounds (coin-heaven), it has learned, as expected, to perfectly gather the coins and to not die, at least not on purpose.
        It has also no problem adapting to scenarios where the coins are spread thinner:

            "coin-slightly-less-heaven": {
                "CRATE_DENSITY": 0,
                "COIN_COUNT": 9
            }

        #### Second training session: (stats/ts-2-2-1.csv)

        With the new rewards:

            e.INVALID_ACTION: -0.05,
            e.MOVED_LEFT: -0.01,
            e.MOVED_RIGHT: -0.01,
            e.MOVED_UP: -0.01,
            e.MOVED_DOWN: -0.01,
            e.WAITED: -0.02,
            e.KILLED_SELF: -5,
            e.BOMB_DROPPED: -0.05,
            e.USELESS_BOMB: -0.2,
            e.CRATE_COMBO: 0.5,
            e.COIN_FOUND: 0.2,
            e.SURVIVED_ROUND: 5,
            e.COIN_COLLECTED: 1,
            e.CRATE_DESTROYED: 0.3,
            # Custom events
            e.WARM: - 0.05,
            e.HOT: -0.05,
            e.BOILING: -0.05,
            e.FRESHENED_UP: 0.05

        We want to penalize movement, balance out bomb placement (placing a bomb means bein WARM, which gives the same penalty as the 
        FRESHENED_UP reward, so they cancel out). Therefore, it should ideally not learn to get stuck in the bomb-placement-escape loop.
        We also don't want it to place bombs stupidly, so we penalize bomb_placement in general, particularly useless bombs and also reward an extra
        0.5 points for breaking many (3+) bombs at once.



        The coin-collecting agent is saved in coin-heaven-1-1-1.pt and we move on with training. We will from here on perform different approaches to training:
        First, we will directly train the agent with the next scenario: loot-crate. In dqn_lapsus_v2, we will train the agent from scratch but already in an
        environment with sufficient crates.




