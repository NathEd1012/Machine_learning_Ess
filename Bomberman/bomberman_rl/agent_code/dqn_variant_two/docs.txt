Documentation for Variant Two of the DQN:

In an elimination type matchup, we will select different features and train them on Variant One and Variant Two.
The features which lead to the best learned behavior will move on to the next stage of training.

On Phase 1: We want to teach the agent to walk and to avoid walking directly into walls. Therefore, we need to provide 
it with knowledge about the immediate neighboring walls and punish it for performing invalid actions. We will monitor the 
amount of Invalid Actions the agent performs with consecutive training iterations. For this first stage, we remove 'BOMB'
from the possible actions, to avoid having that as an Invalid Action.

    ################## Variant 2.1.1

    The features vector is a four dimensional vector which only indicates whether a move up, right, down, left is valid, i.e.
    if game_state['field'] == 0 for that position. 

    In the first variant, we will punish Invalid Action with -1 and reward any kind of movement with 0.02, so that the agent 
    learns to explore. We will need to take precaution with rewarding movement, as we don't want to train the agent to explore 
    recklessly in the future. We will also punish waiting with -0.1 to keep the agent from staying in the same tile throughout
    the game.

    To train the agent, we created a custom scenario 

        "mostly-empty": {
        "CRATE_DENSITY": 0.2,
        "COIN_COUNT": 0
        }

    to account for crates, but also permit somewhat free exploration. With the given feature, crates are handled the same way as 
    walls.

    The rewards are set as:

        e.INVALID_ACTION: -1,
        e.MOVED_LEFT: 0.02,
        e.MOVED_RIGHT: 0.02,
        e.MOVED_UP: 0.02,
        e.MOVED_DOWN: 0.02,
        e.WAITED: -0.05,

    ---------> Results: 
        Agent seems to convert to a strategy where it doesn't try going into walls, however, it gets stuck in a loop of
        right-left or up-down, since it doesn't have any incentive to explore.

        #### First training session: 2024-09-10 21:41:40 (stats/ts-2-1-1-1.csv)
        dqn_variant_two appears to converge much faster to the predicted 14.4 invalid moves per round dictated by our 
        greedy epsilon strategy. After less than 100 training iterations, it has learned not to crash into walls.

        We delete the model (models/2-1-1-1) and begin again to confirm that this is not an anomaly.

        #### Second training session: 2024-09-10 22:39:56 (stats/ts-2-1-1-2.csv)
        Initially, it appears as though the agent is performing random actions throughout and crashing all the time into
        walls, completely contradicting the smooth behavior it showed in the first session. After 1000 training rounds, 
        it converges for a while to a suboptimal policy at around 30 invalid actions per round. A short while later it 
        converges to the expected ~14.

        Contrary to the first session, at no point in the 2400 training rounds does dqn_v2 learn not to wait.

        We delete the model (models/2-1-1-2.pt) and begin again for peace of mind.

        #### Third training session: 2024-09-10 23:04:23 (stats/ts-2-1-1-3.csv)
        dqn_v2 appears to learn quickly, that invalid actions are bad (after 100 rounds), but isn't that convinced. The 
        value fluctuates drastically and after 300 rounds, it skyrockets again for another 100 rounds before it settles on the optimal policy.
        However, it learns to WAIT for every move.

        We delete the model (models/2-1-1-3.pt) and begin again for peace of mind.

        #### Fourth training session: 2024-09-10 23:25:58 (stats/ts-2-1-1-4.csv)
        The agent learns very quickly to avoid invalid actions and also relatively quickly not to wait for the optimal policy.

    ################## Variant 2.1.2

    The model doesn't seem to converge reliably to an optimal policy, so we will try using more aggressive rewards for walking 
    and waiting.

        e.INVALID_ACTION: -1,
        e.MOVED_LEFT: 0.1,
        e.MOVED_RIGHT: 0.1,
        e.MOVED_UP: 0.1,
        e.MOVED_DOWN: 0.1,
        e.WAITED: -0.2,

    ---------> Results: 
        #### First training session: 2024-09-11 10:03:42 (stats/ts-2-1-2-1.csv)

        With these aggressive rewards, the agent seems to very quickly converge to the optimal policy, it neither waits much nor 
        performs invalid actions. A replay shows as expected, that the agent flips between one tile and the next, as it has no 
        incentive to explore.

        We delete the model (models/2-1-2-1.pt) and begin again.
        
        #### Second training session: 2024-09-11 10:23:20 (stats/ts-2-1-2-2.csv)

        A similar behavior is shown as in the first session, showing a quick convergence to the optimal solution. It could be, 
        that, at least initially, higher rewards / punishments for movement and waiting make it learn the optimal policy more 
        reliably.

        We delete the model (models/2-1-2-2.pt) and begin again.

        #### Third training session: 2024-09-11 10:23:15 (stats/ts-2-1-2-3.csv)

        As the model seems to converge quicker, we reduce the number of training rounds. While dqn_v1 struggles to converge to an optimal 
        optimal policy, dqn_v2 shows the same steep convergence to the optimal policy.

        We delete the model (models/2-1-2-3.pt) and begin again.

        #### Fourth training session: 2024-09-11 15:18:30 (stats/ts-2-1-2-4.csv)

        dqn_v2 shows in most cases a consistent learning behavior and converges to the optimal solution within few iterations of the game.
        Additionally, more aggressive rewards and punishments for movement and waiting make it converge more reliably, although even with
        a sparse reward system it managed to perform good as well.

        The version we keep for the next steps is:

        rewards:

            e.INVALID_ACTION: -1,
            e.MOVED_LEFT: 0.1,
            e.MOVED_RIGHT: 0.1,
            e.MOVED_UP: 0.1,
            e.MOVED_DOWN: 0.1,
            e.WAITED: -0.2,

        feature:

            ################## Variant 2.1.1
            def get_neighboring_tiles(own_position, game_state):

                field = game_state['field']
                x, y = own_position
                rows, cols = field.shape

                tile_up = 1 if field[x][y - 1] == 0 else 0
                tile_down = 1 if field[x][y + 1] == 0 else 0
                tile_right = 1 if field[x + 1][y] == 0 else 0
                tile_left = 1 if field[x - 1][y] == 0 else 0

                neighboring_tiles = [tile_up, tile_right, tile_down, tile_left]

                return neighboring_tiles


################## Variant 1.2

On Phase 2: We want to teach the agent how to avoid walking into bombs it placed itself. So, the agent Should
have access to features that inform it about the explosion map and the impending danger after bombs. Likewise,
we will adjust the rewards to reinforce this behavior.

    ################## Variant 1.2.1

    We will try to incorporate the information about whether or not a bomb is going to explode into the same 4 (+1) 
    features of neighboring_tiles. By including the information of the type of tile it is with the minus sign, we hope 
    that the agent can learn the same behaviors with less features.

    ---------> Results: 
        #### First training session: 2024-09-11 10:03:42 (stats/ts-2-1-2-1.csv)

        #### Second training session: 2024-09-11 20:50:17 (stats/ts-2-2-1-2.csv)

        dqn_v2 learns not to kill itself quite quickly, but it is still waiting most of the time. It points towards a good
        direction, but we want it to learn to place bombs AND avoid them. We will boost the rewards for placing a bomb.

        We delete the model (models/2-2-1-2.pt) and begin again.

        #### Third training session: 2024-09-11 21:05:39 (stats/ts-1-2-1-3.csv)

        Still no learning after 3000 iterations. Kills self immediately. 

            e.INVALID_ACTION: -1,
            e.MOVED_LEFT: 0.1,
            e.MOVED_RIGHT: 0.1,
            e.MOVED_UP: 0.1,
            e.MOVED_DOWN: 0.1,
            e.WAITED: -0.2,
            e.KILLED_SELF: -5,
            e.BOMB_DROPPED: 0.5,
            e.SURVIVED_ROUND: 5

        #### Fourth training session: 2024-09-11 21:05:39 (stats/ts-1-2-1-4.csv)

        We try with more aggressive rewards for surviving the round and offing itself.

            e.INVALID_ACTION: -1,
            e.MOVED_LEFT: 0.1,
            e.MOVED_RIGHT: 0.1,
            e.MOVED_UP: 0.1,
            e.MOVED_DOWN: 0.1,
            e.WAITED: -0.2,
            e.KILLED_SELF: -50,
            e.BOMB_DROPPED: 5,
            e.SURVIVED_ROUND: 50